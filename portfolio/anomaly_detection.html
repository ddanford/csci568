<html>
	<head>
		<title>Anomaly Detection</title>
		<meta name="keywords" content="Data Mining" />
		<meta name="description" content="Data Mining" />
		<meta name="author" content="David Danford" />
		<meta http-equiv="content-type" content="text/html;charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<link rel="stylesheet" href="css/blueprint/screen.css" type="text/css" media="screen, projection" />
		<link rel="stylesheet" href="css/blueprint/print.css" type="text/css" media="print" />
		<link rel="stylesheet" href="css/main.css" type="text/css" media="screen" /> 
		<!--[if IE]>
		  <link rel="stylesheet" href="css/blueprint/ie.css" type="text/css" media="screen, projection">
		<![endif]-->
	</head>
	<body>
		<div class="container">
			<h1>Anomaly Detection</h1>
			<p class="introduction">Anomaly detection refers to the process of finding objects that are very different from the rest of the objects in your dataset.  Anomalies can occur in many different ways.  A human might mistype one attribute while entering the data, an instrument might malfunction causing the data to be incorrect, or it might just be natural variation in an attribute.  The problem is that it's often hard to tell why an object is anomalous.</p>
			<h3>Statistical Approach</h3>
			<p>A statistical approach is very similar to our Bayesian classifier.  If we assign every object a percent chance of being part of a class, and it has a low probability of being a member of every class then we can call it an anomaly.  One problem with this approach is that it requires some foreknowledge of the classes that exist, so it can't be done in conjunction with a clustering algorithm.</p>
			<p>Other statistical approaches involve probability distributions.  Given a normal distribution, if an object occurs in the first few percentiles or last few percentiles, it can be called an anomaly.  Or, given a dataset a distribution can be generated from the objects and anything outside of a few standard deviations can be labeled an anomaly.</p>
			<p>Statistical approaches can perform badly when given high dimensional datasets.  How does one generate a standard deviation of hundreds of attributes?  It is also meaningless to find the standard deviation of a binary attribute.  The main advantage of these approaches is that Statistics has had longer as a field to work through these problems, and there's lots of background in the area.</p>
			<h3>Proximity Approach</h3>
			<p>A proximity based approach is similar to our nearest neighbor classification, except now we want to know the distance instead of the class.  In general, these approaches set some proximity threshold.  If the object's kth nearest neighbor is not within that proximity threshold then we call it an anomaly.</p>
			<p>This approach is extremely time intensive, due to the need to calculate the distance from every point to every other point.  It's also very sensitive to the parameters.  If a very large k is given, you might exceed the size of small clusters and end up classifying them all as anomalies.  If a very small k is given, you may not find any anomalies.  The proximity threshold also needs to be chosen based on what k is given.  If a very small proximity is given with a large k, almost everything will be an anomaly.  It is also very sensitive to density.  If one cluster is very dense, and another cluster is relatively spread out, then you might identify the spread out cluster as anomalies, or not catch anomalies near the smaller cluster.</p>
			<p>The main advantage of the proximity approaches is the ease of creation.  Programmers can easily understand it and generate the necessary data programmaticly, unlike statistical approaches that require a deep knowledge of complex statistics.  It's also much easier to determine a proximity that might be appropriate than to find the distribution of the dataset, especially across large datasets.</p>
			<h3>Density Approach</h3>
			<p>In general, a density based approach says that anomalies will occur in extremely low density areas.  There are a few ways we can do density based anomaly detection.  The first, and perhaps easiest, is using DBScan.  DBScan already identifies objects in low density regions, and classifies them as noise.  We can simply take all noise points from DBScan and call them anomalies.</p>
			<p>Another approach is to use a k nearest neighbor algorithm to calculate the average distance to a point's k nearest neighbors.  If this number is high, then the object is, on average, very far from other points.  Or, in an area of low density.  If it's very low, then the point is very close to its neighbors â€“ an area of high density.</p>
			<p>The upsides and downsides of these approaches are the same as the algorithms they're based on.  DBScan doesn't do well when given data with varying densities, and KNN algorithms are very expensive to calculate.  DBScan is nice because it can essentially perform clustering and anomaly detection at the same time, and KNN is just simple to understand.</p>
			<h3>Clustering Approach</h3>
			<p>There are several different clustering approaches to identifying anomalies.  The first, and perhaps simplest, is to say that any cluster that has fewer items in it than some threshold contains anomalies.  This approach has the advantage of being simple, however the clustering algorithm used would need to be kept in mind when looking at the anomalies.  Perhaps it's just a small cluster, and not actually anomalous.</p>
			<p>Another approach is to look at the sum squared error (SSE) of a cluster.  If the SSE is above some threshold, then we can look at the points inside that cluster to see which of them contributes highly to the SSE.  Items that are far away from the centroid can be classified as anomalies.</p>
			<p>These approaches can have good time and space complexities depending on the clustering algorithm used.  For example, K-means clustering is very fast and creates clusters in near linear time and space.  We can then give every object an anomaly rating in linear time, to identify which objects may be anomalous.</p>
			<p>Finally, since this is the last topic portfolio, it is important to stress again the human involvement in the data mining process.  In anomaly detection specifically, we can't just classify objects as anomalies and not try to identify what made them anomalous in the first place.  The human brain is able to catch things that a computer can't, and data mining tools and methods should be viewed as what they are: tools and methods.  They can't replace the human who spent the time becoming familiar with the dataset, and understands the potential sources of anomalies.</p>
			<h3>Conclusion</h3>
		</div>
	</body>
</html>
